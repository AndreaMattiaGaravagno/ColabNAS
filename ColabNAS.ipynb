{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ColabNAS code"
      ],
      "metadata": {
        "id": "3wS5fVIgvMnM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO9151Q_vBqT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import datetime\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "\n",
        "class ColabNAS :\n",
        "    architecture_name = 'resulting_architecture'\n",
        "    def __init__(self, max_RAM, max_Flash, max_MACC, path_to_training_set, val_split, cache=False, input_shape=(50,50,3), save_path='./') :\n",
        "        self.learning_rate = 1e-3\n",
        "        self.batch_size = 128\n",
        "        self.epochs = 100 #minimum 2\n",
        "\n",
        "        self.max_MACC = max_MACC\n",
        "        self.max_Flash = max_Flash\n",
        "        self.max_RAM = max_RAM\n",
        "        self.path_to_training_set = path_to_training_set\n",
        "        self.num_classes = len(next(os.walk(path_to_training_set))[1])\n",
        "        self.val_split = val_split\n",
        "        self.cache = cache\n",
        "        self.input_shape = input_shape\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.path_to_trained_models = f\"{self.save_path}/trained_models\"\n",
        "        os.makedirs(self.path_to_trained_models)\n",
        "\n",
        "        self.load_training_set()\n",
        "\n",
        "    # k number of kernels of the first convolutional layer\n",
        "    # c number of cells added upon the first convolutional layer\n",
        "    # pre-processing pipeline not included in MACC computation\n",
        "    def Model(self, k, c) :\n",
        "        kernel_size = (3,3)\n",
        "        pool_size = (2,2)\n",
        "        pool_strides = (2,2)\n",
        "\n",
        "        number_of_cells_limited = False\n",
        "        number_of_mac = 0\n",
        "\n",
        "        inputs = tf.keras.Input(shape=self.input_shape)\n",
        "\n",
        "        #preprocessing pipeline\n",
        "        x = tf.keras.layers.RandomFlip('horizontal')(inputs)\n",
        "        x = tf.keras.layers.RandomRotation(0.2, fill_mode='constant', interpolation='bilinear')(x)\n",
        "        x = tf.keras.layers.Rescaling(1./255)(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        #convolutional base\n",
        "        n = k\n",
        "        multiplier = 2\n",
        "\n",
        "        #first convolutional layer\n",
        "        c_in = self.input_shape[2]\n",
        "        x = tf.keras.layers.Conv2D(n, kernel_size, activation='relu', padding='same')(x)\n",
        "        number_of_mac = number_of_mac + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        #adding cells\n",
        "        for i in range(1, c + 1) :\n",
        "            if x.shape[1] <= 1 or x.shape[2] <= 1 :\n",
        "                number_of_cells_limited = True\n",
        "                break;\n",
        "            n = np.ceil(n * multiplier)\n",
        "            multiplier = multiplier - 2**-i\n",
        "            x = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_strides, padding='valid')(x)\n",
        "            c_in = x.shape[3]\n",
        "            x = tf.keras.layers.Conv2D(n, kernel_size, activation='relu', padding='same')(x)\n",
        "            number_of_mac = number_of_mac + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        #classifier\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        input_shape = x.shape[1]\n",
        "        x = tf.keras.layers.Dense(n, activation='relu')(x)\n",
        "        number_of_mac = number_of_mac + (input_shape * x.shape[1])\n",
        "        outputs = tf.keras.layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "        number_of_mac = number_of_mac + (x.shape[1] * outputs.shape[1])\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        model.compile(optimizer=opt,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        return model, number_of_mac, number_of_cells_limited\n",
        "\n",
        "    def load_training_set(self):\n",
        "        if 3 == self.input_shape[2] :\n",
        "            color_mode = 'rgb'\n",
        "        elif 1 == self.input_shape[2] :\n",
        "            color_mode = 'grayscale'\n",
        "\n",
        "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        if self.cache :\n",
        "            self.train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        else :\n",
        "            self.train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    def quantize_model_uint8(self) :\n",
        "        def representative_dataset():\n",
        "            for data in self.train_ds.rebatch(1).take(150) :\n",
        "                yield [tf.dtypes.cast(data[0], tf.float32)]\n",
        "\n",
        "        model = tf.keras.models.load_model(f\"{self.path_to_trained_models}/{self.model_name}.h5\")\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.representative_dataset = representative_dataset\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.uint8\n",
        "        converter.inference_output_type = tf.uint8\n",
        "        tflite_quant_model = converter.convert()\n",
        "\n",
        "        with open(f\"{self.path_to_trained_models}/{self.model_name}.tflite\", 'wb') as f:\n",
        "            f.write(tflite_quant_model)\n",
        "\n",
        "        os.remove(f\"{self.path_to_trained_models}/{self.model_name}.h5\")\n",
        "\n",
        "    def evaluate_flash_and_peak_RAM_occupancy(self) :\n",
        "        #quantize model to evaluate its peak RAM occupancy and its Flash occupancy\n",
        "        self.quantize_model_uint8()\n",
        "\n",
        "        #evaluate its peak RAM occupancy and its Flash occupancy using STMicroelectronics' X-CUBE-AI\n",
        "        proc = subprocess.Popen([\"./stm32tflm\", f\"{self.path_to_trained_models}/{self.model_name}.tflite\"], stdout=subprocess.PIPE)\n",
        "        try:\n",
        "            outs, errs = proc.communicate(timeout=15)\n",
        "            Flash, RAM = re.findall(r'\\d+', str(outs))\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "            outs, errs = proc.communicate()\n",
        "            print(\"stm32tflm error\")\n",
        "            exit()\n",
        "\n",
        "        return int(Flash), int(RAM)\n",
        "\n",
        "    def evaluate_model_process(self, k, c) :\n",
        "        if k > 0 :\n",
        "            self.model_name = f\"k_{k}_c_{c}\"\n",
        "            print(f\"\\n{self.model_name}\\n\")\n",
        "            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "                f\"{self.path_to_trained_models}/{self.model_name}.h5\", monitor='val_accuracy',\n",
        "                verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "            model, MACC, number_of_cells_limited = self.Model(k, c)\n",
        "            #One epoch of training must be done before quantization, which is needed to evaluate RAM and Flash occupancy\n",
        "            model.fit(self.train_ds, epochs=1, validation_data=self.validation_ds, validation_freq=1)\n",
        "            model.save(f\"{self.path_to_trained_models}/{self.model_name}.h5\")\n",
        "            Flash, RAM = self.evaluate_flash_and_peak_RAM_occupancy()\n",
        "            print(f\"\\nRAM: {RAM},\\t Flash: {Flash},\\t MACC: {MACC}\\n\")\n",
        "            if MACC <= self.max_MACC and Flash <= self.max_Flash and RAM <= self.max_RAM and not number_of_cells_limited :\n",
        "                hist = model.fit(self.train_ds, epochs=self.epochs - 1, validation_data=self.validation_ds, validation_freq=1, callbacks=[checkpoint])\n",
        "                self.quantize_model_uint8()\n",
        "            return {'k': k,\n",
        "                    'c': c if not number_of_cells_limited else \"Not feasible\",\n",
        "                    'RAM': RAM if RAM <= self.max_RAM else \"Outside the upper bound\",\n",
        "                    'Flash': Flash if Flash <= self.max_Flash else \"Outside the upper bound\",\n",
        "                    'MACC': MACC if MACC <= self.max_MACC else \"Outside the upper bound\",\n",
        "                    'max_val_acc':\n",
        "                    np.around(np.amax(hist.history['val_accuracy']), decimals=3)\n",
        "                    if 'hist' in locals() else -3}\n",
        "        else :\n",
        "            return{'k': 'unfeasible', 'c': c, 'max_val_acc': -3}\n",
        "\n",
        "    def explore_num_cells(self, k) :\n",
        "        previous_architecture = {'k': -1, 'c': -1, 'max_val_acc': -2}\n",
        "        current_architecture = {'k': -1, 'c': -1, 'max_val_acc': -1}\n",
        "        c = -1\n",
        "        k = int(k)\n",
        "\n",
        "        while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
        "            previous_architecture = current_architecture\n",
        "            c = c + 1\n",
        "            self.model_counter = self.model_counter + 1\n",
        "            current_architecture = self.evaluate_model_process(k, c)\n",
        "            print(f\"\\n\\n\\n{current_architecture}\\n\\n\\n\")\n",
        "        return previous_architecture\n",
        "\n",
        "    def search(self) :\n",
        "        self.model_counter = 0\n",
        "        epsilon = 0.005\n",
        "        k0 = 4\n",
        "\n",
        "        start = datetime.datetime.now()\n",
        "\n",
        "        k = k0\n",
        "        previous_architecture = self.explore_num_cells(k)\n",
        "        k = 2 * k\n",
        "        current_architecture = self.explore_num_cells(k)\n",
        "\n",
        "        if (current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
        "            previous_architecture = current_architecture\n",
        "            k = 2 * k\n",
        "            current_architecture = self.explore_num_cells(k)\n",
        "            while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc'] + epsilon) :\n",
        "                previous_architecture = current_architecture\n",
        "                k = 2 * k\n",
        "                current_architecture = self.explore_num_cells(k)\n",
        "        else :\n",
        "            k = k0 / 2\n",
        "            current_architecture = self.explore_num_cells(k)\n",
        "            while(current_architecture['max_val_acc'] >= previous_architecture['max_val_acc']) :\n",
        "                previous_architecture = current_architecture\n",
        "                k = k / 2\n",
        "                current_architecture = self.explore_num_cells(k)\n",
        "\n",
        "        resulting_architecture = previous_architecture\n",
        "\n",
        "        end = datetime.datetime.now()\n",
        "\n",
        "        if (resulting_architecture['max_val_acc'] > 0) :\n",
        "            resulting_architecture_name = f\"k_{resulting_architecture['k']}_c_{resulting_architecture['c']}.tflite\"\n",
        "            self.path_to_resulting_architecture = f\"{self.save_path}/resulting_architecture_{resulting_architecture_name}\"\n",
        "            os.rename(f\"{self.path_to_trained_models}/{resulting_architecture_name}\", self.path_to_resulting_architecture)\n",
        "            os.system(f\"rm -rf {self.path_to_trained_models}\")\n",
        "            print(f\"\\nResulting architecture: {resulting_architecture}\\n\")\n",
        "        else :\n",
        "            print(f\"\\nNo feasible architecture found\\n\")\n",
        "        print(f\"Elapsed time (search): {end-start}\\n\")\n",
        "\n",
        "        return self.path_to_resulting_architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a useful function for testing tflite models"
      ],
      "metadata": {
        "id": "TdK-7Hj4vzL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tflite_model(path_to_resulting_architecture, test_ds) :\n",
        "    interpreter = tf.lite.Interpreter(path_to_resulting_architecture)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    output = interpreter.get_output_details()[0]  # Model has single output.\n",
        "    input = interpreter.get_input_details()[0]  # Model has single input.\n",
        "\n",
        "    correct = 0\n",
        "    wrong = 0\n",
        "\n",
        "    for image, label in test_ds :\n",
        "        # Check if the input type is quantized, then rescale input data to uint8\n",
        "        if input['dtype'] == tf.uint8:\n",
        "            input_scale, input_zero_point = input[\"quantization\"]\n",
        "            image = image / input_scale + input_zero_point\n",
        "        input_data = tf.dtypes.cast(image, tf.uint8)\n",
        "        interpreter.set_tensor(input['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "        if label.numpy().argmax() == interpreter.get_tensor(output['index']).argmax() :\n",
        "            correct = correct + 1\n",
        "        else :\n",
        "            wrong = wrong + 1\n",
        "    print(f\"\\nTflite model test accuracy: {correct/(correct+wrong)}\")\n"
      ],
      "metadata": {
        "id": "U5V1U9fzvrFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the stm32tflm script in the files folder of Google Colaboratory's VM"
      ],
      "metadata": {
        "id": "J7zgHegxw2W7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable its execution"
      ],
      "metadata": {
        "id": "BRhFldu7xJ32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x stm32tflm"
      ],
      "metadata": {
        "id": "OD11DAd3xMNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example of usage"
      ],
      "metadata": {
        "id": "p3JwWjuGvSwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "input_shape = (50,50,3)\n",
        "\n",
        "#target: STM32L412KBU3\n",
        "#273 CoreMark, 40 kiB RAM, 128 kiB Flash\n",
        "peak_RAM_upper_bound = 40960\n",
        "Flash_upper_bound = 131072\n",
        "MACC_upper_bound = 2730000 #CoreMark * 1e4\n",
        "\n",
        "#Each dataset must comply with the following structure\n",
        "#main_directory/\n",
        "#...class_a/\n",
        "#......a_image_1.jpg\n",
        "#......a_image_2.jpg\n",
        "#...class_b/\n",
        "#......b_image_1.jpg\n",
        "#......b_image_2.jpg\n",
        "path_to_training_set = './path/to/training/set'\n",
        "val_split = 0.3\n",
        "\n",
        "#whether or not to cache datasets in memory\n",
        "#if the dataset cannot fit in the main memory, the application will crash\n",
        "cache = True\n",
        "\n",
        "#where to save results\n",
        "save_path = './path/to/resulting/architecture'\n",
        "\n",
        "#to show the GPU used\n",
        "!nvidia-smi\n",
        "\n",
        "colabNAS = ColabNAS(peak_RAM_upper_bound, Flash_upper_bound, MACC_upper_bound, path_to_training_set, val_split, cache, input_shape, save_path=save_path)\n",
        "\n",
        "#search\n",
        "path_to_tflite_model = colabNAS.search()\n",
        "\n",
        "#test\n",
        "path_to_test_set = './path/to/test/set'\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory= path_to_test_set,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=1,\n",
        "    image_size=input_shape[0:2],\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_tflite_model(path_to_tflite_model, test_ds)"
      ],
      "metadata": {
        "id": "a3uDiwf7vVBX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Before starting:\n",
        "* This notebook is meant to run on Google Colaboratory;\n",
        "* It requires you to download a third party software, otherwise it will not run. (I know it's tedious but I don't own the program and so I can't share it with you);\n",
        "* I suggest to use an environment with a GPU.\n"
      ],
      "metadata": {
        "id": "S7pJZhyyHImS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preliminaries"
      ],
      "metadata": {
        "id": "JoivgyMxAx73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install 'tensorflow-model-optimization' for Quantization Aware Training (QAT)"
      ],
      "metadata": {
        "id": "aHlEjP0TA53q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R8PtDnQ_Dat"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get 'stm32tflm' software"
      ],
      "metadata": {
        "id": "b7biWvQrBB2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download 'X-CUBE-AI-Linux' package from https://www.st.com/en/embedded-software/x-cube-ai.html;\n",
        "* Extract the 'stm32tflm' executable from the downloaded package;\n",
        "* Put it in the folder you're working (usually '/content/' for Google Colaboratory)."
      ],
      "metadata": {
        "id": "UMu6x4Fi_bte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable the execution of the 'stm32tflm' program"
      ],
      "metadata": {
        "id": "tPzOk0TMAg4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x stm32tflm"
      ],
      "metadata": {
        "id": "ImbM8NNWARqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ColabNAS code"
      ],
      "metadata": {
        "id": "7EWcc1S1BVuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import datetime\n",
        "import shutil\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "\n",
        "class ColabNAS :\n",
        "    architecture_name = 'resulting_architecture'\n",
        "    def __init__(self, max_RAM, max_Flash, max_MACC, path_to_training_set, val_split, cache=False, input_shape=(50,50,3), save_path='.', path_to_stm32tflm='/content/stm32tflm') :\n",
        "        self.learning_rate = 1e-3\n",
        "        self.batch_size = 128\n",
        "        self.epochs = 100 #minimum 2\n",
        "\n",
        "        self.max_MACC = max_MACC\n",
        "        self.max_Flash = max_Flash\n",
        "        self.max_RAM = max_RAM\n",
        "        self.path_to_training_set = path_to_training_set\n",
        "        self.num_classes = len(next(os.walk(path_to_training_set))[1])\n",
        "        self.val_split = val_split\n",
        "        self.cache = cache\n",
        "        self.input_shape = input_shape\n",
        "        self.save_path = Path(save_path)\n",
        "\n",
        "        self.path_to_trained_models = self.save_path / \"trained_models\"\n",
        "        self.path_to_trained_models.mkdir(parents=True)\n",
        "\n",
        "        self.path_to_stm32tflm = Path(path_to_stm32tflm)\n",
        "\n",
        "        self.load_training_set()\n",
        "\n",
        "    # k number of kernels of the first convolutional layer\n",
        "    # c number of cells added upon the first convolutional layer\n",
        "    # pre-processing pipeline not included in MACC computation\n",
        "    def Model(self, k, c) :\n",
        "        kernel_size = (3,3)\n",
        "        pool_size = (2,2)\n",
        "        pool_strides = (2,2)\n",
        "\n",
        "        number_of_cells_limited = False\n",
        "        number_of_mac = 0\n",
        "\n",
        "        inputs = keras.Input(shape=self.input_shape)\n",
        "\n",
        "        #convolutional base\n",
        "        n = int(k)\n",
        "        multiplier = 2\n",
        "\n",
        "        #first convolutional layer\n",
        "        c_in = self.input_shape[2]\n",
        "        x = keras.layers.Conv2D(n, kernel_size, padding='same')(inputs)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "        number_of_mac = number_of_mac + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        #adding cells\n",
        "        for i in range(1, c + 1) :\n",
        "            if x.shape[1] <= 1 or x.shape[2] <= 1 :\n",
        "                number_of_cells_limited = True\n",
        "                break;\n",
        "            n = int(np.ceil(n * multiplier))\n",
        "            multiplier = multiplier - 2**-i\n",
        "            x = keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_strides, padding='valid')(x)\n",
        "            c_in = x.shape[3]\n",
        "            x = keras.layers.Conv2D(n, kernel_size, padding='same')(x)\n",
        "            x = keras.layers.BatchNormalization()(x)\n",
        "            x = keras.layers.ReLU()(x)\n",
        "            number_of_mac = number_of_mac + (c_in * kernel_size[0] * kernel_size[1] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "\n",
        "        #classifier\n",
        "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "        input_shape = x.shape[1]\n",
        "        x = keras.layers.Dense(n)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "        number_of_mac = number_of_mac + (input_shape * x.shape[1])\n",
        "        x = keras.layers.Dense(self.num_classes)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        outputs = keras.layers.Softmax()(x)\n",
        "        number_of_mac = number_of_mac + (x.shape[1] * outputs.shape[1])\n",
        "\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        model.compile(optimizer=opt,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        return model, number_of_mac, number_of_cells_limited\n",
        "\n",
        "    def load_training_set(self):\n",
        "        if 3 == self.input_shape[2] :\n",
        "            color_mode = 'rgb'\n",
        "        elif 1 == self.input_shape[2] :\n",
        "            color_mode = 'grayscale'\n",
        "\n",
        "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            directory= self.path_to_training_set,\n",
        "            labels='inferred',\n",
        "            label_mode='categorical',\n",
        "            color_mode=color_mode,\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.input_shape[0:2],\n",
        "            shuffle=True,\n",
        "            seed=11,\n",
        "            validation_split=self.val_split,\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        data_augmentation = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "            tf.keras.layers.RandomRotation(0.2, fill_mode='constant', interpolation='bilinear'),\n",
        "            #tf.keras.layers.Rescaling(1./255)\n",
        "            ])\n",
        "\n",
        "        if self.cache :\n",
        "            self.train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        else :\n",
        "            self.train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "            self.validation_ds = validation_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    def quantize_model_uint8(self) :\n",
        "        def representative_dataset():\n",
        "            for data in self.train_ds.rebatch(1).take(150) :\n",
        "                yield [tf.dtypes.cast(data[0], tf.float32)]\n",
        "\n",
        "        model = tf.keras.models.load_model(self.path_to_trained_models / f\"{self.model_name}.h5\")\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.representative_dataset = representative_dataset\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.uint8\n",
        "        converter.inference_output_type = tf.uint8\n",
        "        tflite_quant_model = converter.convert()\n",
        "\n",
        "        with open(self.path_to_trained_models / f\"{self.model_name}.tflite\", 'wb') as f:\n",
        "            f.write(tflite_quant_model)\n",
        "\n",
        "        (self.path_to_trained_models / f\"{self.model_name}.h5\").unlink()\n",
        "\n",
        "    def evaluate_flash_and_peak_RAM_occupancy(self) :\n",
        "        #quantize model to evaluate its peak RAM occupancy and its Flash occupancy\n",
        "        self.quantize_model_uint8()\n",
        "\n",
        "        #evaluate its peak RAM occupancy and its Flash occupancy using STMicroelectronics' X-CUBE-AI\n",
        "        proc = subprocess.Popen([self.path_to_stm32tflm, self.path_to_trained_models / f\"{self.model_name}.tflite\"], stdout=subprocess.PIPE)\n",
        "        try:\n",
        "            outs, errs = proc.communicate(timeout=15)\n",
        "            Flash, RAM = re.findall(r'\\d+', str(outs))\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "            outs, errs = proc.communicate()\n",
        "            print(\"stm32tflm error\")\n",
        "            exit()\n",
        "\n",
        "        return int(Flash), int(RAM)\n",
        "\n",
        "    def evaluate_model_process(self, k, c) :\n",
        "        if k > 0 :\n",
        "            self.model_name = f\"k_{k}_c_{c}\"\n",
        "            print(f\"\\n{self.model_name}\\n\")\n",
        "            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "                str(self.path_to_trained_models / f\"{self.model_name}.h5\"), monitor='val_accuracy',\n",
        "                verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "            model, MACC, number_of_cells_limited = self.Model(k, c)\n",
        "            #One epoch of training must be done before quantization, which is needed to evaluate RAM and Flash occupancy\n",
        "            model.fit(self.train_ds, epochs=1, validation_data=self.validation_ds, validation_freq=1)\n",
        "            model.save(self.path_to_trained_models / f\"{self.model_name}.h5\")\n",
        "            Flash, RAM = self.evaluate_flash_and_peak_RAM_occupancy()\n",
        "            print(f\"\\nRAM: {RAM},\\t Flash: {Flash},\\t MACC: {MACC}\\n\")\n",
        "            if MACC <= self.max_MACC and Flash <= self.max_Flash and RAM <= self.max_RAM and not number_of_cells_limited :\n",
        "                hist = model.fit(self.train_ds, epochs=self.epochs - 1, validation_data=self.validation_ds, validation_freq=1, callbacks=[checkpoint])\n",
        "                self.quantize_model_uint8()\n",
        "            return {'k': k,\n",
        "                    'c': c if not number_of_cells_limited else \"Not feasible\",\n",
        "                    'RAM': RAM if RAM <= self.max_RAM else \"Outside the upper bound\",\n",
        "                    'Flash': Flash if Flash <= self.max_Flash else \"Outside the upper bound\",\n",
        "                    'MACC': MACC if MACC <= self.max_MACC else \"Outside the upper bound\",\n",
        "                    'max_val_acc':\n",
        "                    np.around(np.amax(hist.history['val_accuracy']), decimals=3)\n",
        "                    if 'hist' in locals() else -3}\n",
        "        else :\n",
        "            return{'k': 'unfeasible', 'c': c, 'max_val_acc': -3}\n",
        "\n",
        "    def explore_num_cells(self, k) :\n",
        "        previous_architecture = {'k': -1, 'c': -1, 'max_val_acc': -2}\n",
        "        current_architecture = {'k': -1, 'c': -1, 'max_val_acc': -1}\n",
        "        c = -1\n",
        "        k = int(k)\n",
        "\n",
        "        while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
        "            previous_architecture = current_architecture\n",
        "            c = c + 1\n",
        "            self.model_counter = self.model_counter + 1\n",
        "            current_architecture = self.evaluate_model_process(k, c)\n",
        "            print(f\"\\n\\n\\n{current_architecture}\\n\\n\\n\")\n",
        "        return previous_architecture\n",
        "\n",
        "    def search(self) :\n",
        "        self.model_counter = 0\n",
        "        epsilon = 0.005\n",
        "        k0 = 4\n",
        "\n",
        "        start = datetime.datetime.now()\n",
        "\n",
        "        k = k0\n",
        "        previous_architecture = self.explore_num_cells(k)\n",
        "        k = 2 * k\n",
        "        current_architecture = self.explore_num_cells(k)\n",
        "\n",
        "        if (current_architecture['max_val_acc'] > previous_architecture['max_val_acc']) :\n",
        "            previous_architecture = current_architecture\n",
        "            k = 2 * k\n",
        "            current_architecture = self.explore_num_cells(k)\n",
        "            while(current_architecture['max_val_acc'] > previous_architecture['max_val_acc'] + epsilon) :\n",
        "                previous_architecture = current_architecture\n",
        "                k = 2 * k\n",
        "                current_architecture = self.explore_num_cells(k)\n",
        "        else :\n",
        "            k = k0 / 2\n",
        "            current_architecture = self.explore_num_cells(k)\n",
        "            while(current_architecture['max_val_acc'] >= previous_architecture['max_val_acc']) :\n",
        "                previous_architecture = current_architecture\n",
        "                k = k / 2\n",
        "                current_architecture = self.explore_num_cells(k)\n",
        "\n",
        "        resulting_architecture = previous_architecture\n",
        "\n",
        "        end = datetime.datetime.now()\n",
        "\n",
        "        if (resulting_architecture['max_val_acc'] > 0) :\n",
        "            resulting_architecture_name = f\"k_{resulting_architecture['k']}_c_{resulting_architecture['c']}.tflite\"\n",
        "            self.path_to_resulting_architecture = self.save_path / f\"resulting_architecture_{resulting_architecture_name}\"\n",
        "            (self.path_to_trained_models / f\"{resulting_architecture_name}\").rename(self.path_to_resulting_architecture)\n",
        "            shutil.rmtree(self.path_to_trained_models)\n",
        "            print(f\"\\nResulting architecture: {resulting_architecture}\\n\")\n",
        "        else :\n",
        "            print(f\"\\nNo feasible architecture found\\n\")\n",
        "        print(f\"Elapsed time (search): {end-start}\\n\")\n",
        "\n",
        "        return self.path_to_resulting_architecture"
      ],
      "metadata": {
        "id": "9r2J-EoGBdYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Try ColabNAS!"
      ],
      "metadata": {
        "id": "z_qOEXpzBw6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###download a small dataset"
      ],
      "metadata": {
        "id": "uIK_hFupDt6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
        "data_dir = Path(data_dir).with_suffix('')"
      ],
      "metadata": {
        "id": "POrh-7WTDtkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run HW-NAS\n",
        "target: STM32L412KBU3 (273 CoreMark, 40 kiB RAM, 128 kiB Flash)"
      ],
      "metadata": {
        "id": "s6ckFu2umrTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "input_shape = (50,50,3)\n",
        "\n",
        "#target: STM32L412KBU3\n",
        "#273 CoreMark, 40 kiB RAM, 128 kiB Flash\n",
        "peak_RAM_upper_bound = 40960\n",
        "Flash_upper_bound = 131072\n",
        "MACC_upper_bound = 2730000 #CoreMark * 1e4\n",
        "\n",
        "#Each dataset must comply with the following structure\n",
        "#main_directory/\n",
        "#...class_a/\n",
        "#......a_image_1.jpg\n",
        "#......a_image_2.jpg\n",
        "#...class_b/\n",
        "#......b_image_1.jpg\n",
        "#......b_image_2.jpg\n",
        "path_to_training_set = data_dir\n",
        "val_split = 0.3\n",
        "\n",
        "#whether or not to cache datasets in memory\n",
        "#if the dataset cannot fit in the main memory, the application will crash\n",
        "cache = True\n",
        "\n",
        "#where to save results\n",
        "save_path = '/content/'\n",
        "\n",
        "#to show the GPU used\n",
        "!nvidia-smi\n",
        "\n",
        "colabNAS = ColabNAS(peak_RAM_upper_bound, Flash_upper_bound, MACC_upper_bound, path_to_training_set, val_split, cache, input_shape, save_path=save_path)\n",
        "\n",
        "#search\n",
        "path_to_tflite_model = colabNAS.search()"
      ],
      "metadata": {
        "id": "mRKXWAXqBy7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test the obtained model"
      ],
      "metadata": {
        "id": "OTvk-uoLm60b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a useful function for testing 'tflite' models"
      ],
      "metadata": {
        "id": "fSRUZ3-nCsKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tflite_model(path_to_resulting_model, test_ds) :\n",
        "    interpreter = tf.lite.Interpreter(str(path_to_resulting_model))\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    output = interpreter.get_output_details()[0]  # Model has single output.\n",
        "    input = interpreter.get_input_details()[0]  # Model has single input.\n",
        "\n",
        "    correct = 0\n",
        "    wrong = 0\n",
        "\n",
        "    for i in test_ds :\n",
        "        image, label = i[0], i[1]\n",
        "        # Check if the input type is quantized, then rescale input data to uint8\n",
        "        if input['dtype'] == tf.uint8:\n",
        "            input_scale, input_zero_point = input[\"quantization\"]\n",
        "            image = image / input_scale + input_zero_point\n",
        "        input_data = tf.dtypes.cast(image, tf.uint8)\n",
        "        interpreter.set_tensor(input['index'], input_data)\n",
        "        interpreter.invoke()\n",
        "        if label.numpy().argmax() == interpreter.get_tensor(output['index']).argmax() :\n",
        "            correct = correct + 1\n",
        "        else :\n",
        "            wrong = wrong + 1\n",
        "    print(f\"\\nTflite model test accuracy: {correct/(correct+wrong)}\")"
      ],
      "metadata": {
        "id": "9m4URwA8BlhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download a new image, not used in training"
      ],
      "metadata": {
        "id": "wf8NVCCoI00V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
        "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    sunflower_path, target_size=input_shape[0:2]\n",
        ")"
      ],
      "metadata": {
        "id": "XSC4aMMoI3ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img"
      ],
      "metadata": {
        "id": "H8bRzVk2Kq9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build a dataset containing the new image"
      ],
      "metadata": {
        "id": "tQD-zou2nENU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "img_label = tf.constant([[0., 0., 0., 1., 0.]]) #sunflower\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((img_array, img_label))\n",
        "test_ds = test_ds.batch(1)"
      ],
      "metadata": {
        "id": "OlF5jWrOnCiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test the obtained model"
      ],
      "metadata": {
        "id": "V9I9vWMynLvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tflite_model(path_to_tflite_model, test_ds)"
      ],
      "metadata": {
        "id": "ngoniYv0mOZh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
